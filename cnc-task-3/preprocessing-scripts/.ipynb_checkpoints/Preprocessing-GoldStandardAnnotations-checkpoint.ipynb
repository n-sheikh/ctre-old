{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0578de8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import re\n",
    "import collections\n",
    "import random "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6be3d84d",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_folder_path = '/home/nadia/Documents/CLaC-Lab/TRE/CNC-Task3/data/data/'\n",
    "task_3b_file_path = data_folder_path + 'train_subtask2.csv'\n",
    "task_3a_file_path = data_folder_path + 'train_subtask1.csv'\n",
    "regexes = { 'arg0': [r'<ARG0>', r'</ARG0>'], \n",
    "            'arg1':[r'<ARG1>', r'</ARG1>'], \n",
    "            'sig0':[r'<SIG0>', r'</SIG0>'], \n",
    "            'sig1':[r'<SIG1>', r'</SIG1>'],\n",
    "            'sig2':[r'<SIG2>', r'</SIG2>']}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "a2ba99a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_csv(path):\n",
    "    with open(path) as f:\n",
    "        csvreader = csv.reader(f)\n",
    "        header = next(csvreader)\n",
    "        rows = []\n",
    "        for row in csvreader:\n",
    "            rows.append(row)\n",
    "    return rows, header\n",
    "\n",
    "def extract_ann_offsets(rows, regexes):\n",
    "    ann_offsets = []\n",
    "    for i in range(len(rows)):\n",
    "        ann_string = rows[i][6]\n",
    "        raw_string = rows[i][5]\n",
    "        row_indices = []\n",
    "        row_indices.append(rows[i][1])\n",
    "        for key in regexes.keys():\n",
    "            if re.search(regexes[key][0], ann_string) is not None:\n",
    "                ann_start = re.search(regexes[key][0], ann_string).span()[1]\n",
    "                ann_end = re.search(regexes[key][1], ann_string).span()[0]\n",
    "                substr = ann_string[ann_start:ann_end]\n",
    "                for key in regexes.keys():\n",
    "                    substr = substr.replace(regexes[key][0], \"\")\n",
    "                    substr = substr.replace(regexes[key][1], \"\")\n",
    "                substr = substr.replace(\"(\", \"\\(\")\n",
    "                substr = substr.replace(\")\", \"\\)\")\n",
    "                if re.search(substr, raw_string) is not None:\n",
    "                    substr_indices = re.search(substr, raw_string).span()\n",
    "                    row_indices.append(str(substr_indices[0]) + ':' + str(substr_indices[1]))\n",
    "                else:\n",
    "                    row_indices.append(f'ERR{substr}')\n",
    "            else:\n",
    "                row_indices.append(':')\n",
    "        ann_offsets.append(row_indices)\n",
    "    return ann_offsets\n",
    "\n",
    "def extract_doc_ids(rows, index):\n",
    "    doc_ids = []\n",
    "    for row in rows:\n",
    "        doc_ids.append(row[index])\n",
    "    return doc_ids\n",
    "\n",
    "def generate_span_ann_header(base_header, annotated_doc_ids, regexes):\n",
    "    header = base_header\n",
    "    counter = collections.Counter(annotated_doc_ids)\n",
    "    max_ann_sets = max(counter.values())\n",
    "    for i in range(max_ann_sets):\n",
    "        for key in regexes.keys():\n",
    "            header.append(f\"{i}_{key}\")\n",
    "    return header\n",
    "\n",
    "def extend_base_anns_to_include_span_anns(header, rows, ann_offsets):\n",
    "    task_3a_rows = {}\n",
    "    for row in rows:\n",
    "        task_3a_rows[row[0]] = row \n",
    "    for ann in ann_offsets:\n",
    "        task_3a_rows[ann[0]].extend(ann[1:])\n",
    "    values = list(task_3a_rows.values())\n",
    "    return values\n",
    "\n",
    "def write_csv(path, header,rows):\n",
    "    with open(path, 'w') as f:\n",
    "        csvwriter = csv.writer(f)\n",
    "        csvwriter.writerow(header)\n",
    "        csvwriter.writerows(rows)\n",
    "        \n",
    "def split_ids(ids, pct):\n",
    "    random.seed(42)\n",
    "    train_ids = []\n",
    "    dev_ids = []\n",
    "    for e in ids:\n",
    "        if random.random() < pct:\n",
    "            dev_ids.append(e)\n",
    "        else:\n",
    "            train_ids.append(e)\n",
    "    return dev_ids, train_ids\n",
    "\n",
    "def select_rows(ids, rows):\n",
    "    task_3a_rows = {}\n",
    "    for row in rows:\n",
    "        task_3a_rows[row[0]] = row \n",
    "    data = []\n",
    "    for e in ids:\n",
    "        data.append(task_3a_rows[e])\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "0cbd421a",
   "metadata": {},
   "outputs": [],
   "source": [
    "task_3a_rows, task_3a_header = load_csv(task_3a_file_path)\n",
    "task_3b_rows, task_3b_header = load_csv(task_3b_file_path)\n",
    "ann_offsets = extract_ann_offsets(task_3b_rows, regexes)\n",
    "ann_doc_ids = extract_doc_ids(task_3b_rows, 1)\n",
    "all_doc_ids = extract_doc_ids(task_3a_rows, 0)\n",
    "unann_doc_ids = list(set(all_doc_ids).difference(ann_doc_ids))\n",
    "span_ann_header = generate_span_ann_header(task_3a_header, ann_doc_ids, regexes)\n",
    "task_3a3b_rows = extend_base_anns_to_include_span_anns(span_ann_header, task_3a_rows, ann_offsets)\n",
    "write_csv(data_folder_path + \"subtask3a3b.csv\", span_ann_header, task_3a3b_rows)\n",
    "\n",
    "ann_dev_doc_ids, ann_train_doc_ids = split_ids(set(annotated_doc_ids), 0.15)\n",
    "unann_dev_doc_ids, unann_train_doc_ids = split_ids(unannotated_doc_ids, 0.15)\n",
    "dev_doc_ids = ann_dev_doc_ids + unann_dev_doc_ids\n",
    "train_doc_ids = ann_train_doc_ids + unann_train_doc_ids\n",
    "assert len(dev_doc_ids) + len(train_doc_ids) == len(all_doc_ids)\n",
    "\n",
    "train_task3a3b_rows = select_rows(train_doc_ids, task_3a3b_rows)\n",
    "dev_task3a3b_rows = select_rows(dev_doc_ids, task_3a3b_rows)\n",
    "assert len(dev_task3a3b_rows) + len(train_task3a3b_rows) == len(task_3a3b_rows)\n",
    "\n",
    "write_csv(data_folder_path + \"dev_task3a3b.csv\", span_ann_header, dev_task3a3b_rows)\n",
    "nos_files = int(len(train_task3a3b_rows) / 250)\n",
    "for i in range(nos_files):\n",
    "    if i == nos_files:\n",
    "        write_csv(data_folder_path + f\"train_task3a3b_{i}.csv\", span_ann_header, train_task3a3b_rows[i * 250:])\n",
    "    else:\n",
    "        write_csv(data_folder_path + f\"train_task3a3b_{i}.csv\", span_ann_header, train_task3a3b_rows[i * 250: (i + 1) * 250])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "77c2e093",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2925\n",
      "2925\n"
     ]
    }
   ],
   "source": [
    "print(len(dev_doc_ids) + len(train_doc_ids))\n",
    "print(len(all_doc_ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "a9f18d34",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "7ba5b823",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['index', 'text', 'label', 'agreement', 'num_votes', 'sample_set', '0_arg0', '0_arg1', '0_sig0', '0_sig1', '0_sig2', '1_arg0', '1_arg1', '1_sig0', '1_sig1', '1_sig2', '2_arg0', '2_arg1', '2_sig0', '2_sig1', '2_sig2', '3_arg0', '3_arg1', '3_sig0', '3_sig1', '3_sig2']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "header,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "fce23c29",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_doc_ids = []\n",
    "for row in task_3a_rows.values():\n",
    "    all_doc_ids.append(row[0])\n",
    "unann_indices = list(set(all_doc_ids).difference(set(annotated_doc_ids)))\n",
    "ann_indices = list(set(annotated_doc_ids))\n",
    "assert len(all_doc_ids) == len(unann_indices) + len(ann_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "ecca78d0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6394267984578837"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random.seed(42)\n",
    "if random.random() < 0.2:\n",
    "    dev_unann_indices = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "441e4c7e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f73e4b09",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
