{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "55ca115b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import treebank\n",
    "import regex as re\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9f3db7bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_tree_nltk_treebank(doc_id):\n",
    "    nltk.download('treebank')\n",
    "    for tree in treebank.parsed_sents(doc_id):\n",
    "        tree.pretty_print()\n",
    "\n",
    "def print_leaves_nltk_treebank(doc_id):\n",
    "    nltk.download('treebank')\n",
    "    for tree in treebank.parsed_sents(doc_id):\n",
    "        print(tree.leaves())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8c768c0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Node:\n",
    "    def __init__(self, parent=None, token=None, label=None):\n",
    "      self.children = []\n",
    "      self.parent = parent\n",
    "      self.token = token\n",
    "      self.label = label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "af291c4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_sentence_list_rep_from_ptb_mrg_file(file_path, sen_ind):\n",
    "    with open(file_path) as f:\n",
    "        lines = f.readlines()\n",
    "        indices = []\n",
    "        for i in range(len(lines)):\n",
    "            if lines[i] == '( (S \\n':\n",
    "                indices.append(i)\n",
    "        sentences = []\n",
    "        for i in range(len(indices)):\n",
    "            if i == len(indices) - 1:\n",
    "                sentences.append(lines[indices[i]:])\n",
    "            else:\n",
    "                sentences.append(lines[indices[i]:indices[i+1]])\n",
    "    print(sen_ind)\n",
    "    return sentences[sen_ind]\n",
    "\n",
    "def strip_sentence_list_rep(sen):\n",
    "    stripped_sentence = []\n",
    "    for line in sen:\n",
    "        line = line.strip()\n",
    "        line = re.split('([\\(\\)])', line)\n",
    "        line = [x for x in line if (x != '' and x != ' ')]\n",
    "        stripped_sentence.append(line)\n",
    "    return stripped_sentence\n",
    "\n",
    "def flatten_sentence_list_rep(sen):\n",
    "    fsen = [item for sublist in sen for item in sublist]\n",
    "    fsen = fsen[1:len(fsen) - 1]\n",
    "    return fsen\n",
    "\n",
    "def generate_tree_from_flattened_sentence_list_rep(sen):\n",
    "    tree = Node()\n",
    "    current_node = tree\n",
    "    for i in sen:\n",
    "        if i == '(':\n",
    "            new_node = Node(parent=current_node)\n",
    "            current_node.children.append(new_node)\n",
    "            current_node = new_node\n",
    "        elif i == ')':\n",
    "            current_node = current_node.parent\n",
    "        else:\n",
    "            i_list = i.strip().split(\" \")\n",
    "            current_node.label = i_list[0]\n",
    "            if len(i_list) > 1:\n",
    "                current_node.token = i_list[1]\n",
    "    return tree\n",
    "\n",
    "def generate_ptb_parse_tree(file_path, sen_id):\n",
    "    sentence_list_rep = generate_sentence_list_rep_from_ptb_mrg_file(file_path, sen_id)\n",
    "    stripped_sentence_rep = strip_sentence_list_rep(sentence_list_rep)\n",
    "    flat_sentence_rep = flatten_sentence_list_rep(stripped_sentence_rep)\n",
    "    tree = generate_tree_from_flattened_sentence_list_rep(flat_sentence_rep)\n",
    "    return tree\n",
    "\n",
    "\n",
    "def return_token(leaf):\n",
    "    return leaf.token\n",
    "\n",
    "def return_node(leaf):\n",
    "    return leaf\n",
    "\n",
    "def apply_f_to_leaves_spanned_by_subtree(subtree, f):\n",
    "    unvisited = []\n",
    "    returns = []\n",
    "    children = subtree.children\n",
    "    for child in reversed(children):\n",
    "        unvisited.insert(0, child)\n",
    "    while len(unvisited) != 0:\n",
    "        node = unvisited.pop(0)\n",
    "        if node.token is not None:\n",
    "            returns.append(f(node))\n",
    "        else:\n",
    "            children = node.children\n",
    "            for child in reversed(children):\n",
    "                unvisited.insert(0, child)\n",
    "    return returns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "1bf2a64c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wsj/00/wsj_0001.mrg 0 8 gold join.01 vf--a 0:2-ARG0 7:0-ARGM-MOD 8:0-rel 9:1-ARG1 11:1-ARGM-PRD 15:1-ARGM-TMP\n",
      "\n",
      "['v', 'f', '-', '-', 'a']\n",
      "0\n",
      "{'Pierre': 'ARG0', 'Vinken': 'ARG0', ',': 'ARG0', '61': 'ARG0', 'years': 'ARG0', 'old': 'ARG0', 'will': 'ARGM-MOD', 'join': 'rel', 'the': 'ARG1', 'board': 'ARG1', 'as': 'ARGM-PRD', 'a': 'ARGM-PRD', 'nonexecutive': 'ARGM-PRD', 'director': 'ARGM-PRD', 'Nov.': 'ARGM-TMP', '29': 'ARGM-TMP', '.': None}\n"
     ]
    }
   ],
   "source": [
    "def identify_form(char):\n",
    "    if char == 'i':\n",
    "        return 'infinitive'\n",
    "    elif char == 'g':\n",
    "        return 'gerund'\n",
    "    elif char == 'p':\n",
    "        return 'participle'\n",
    "    elif char == 'v':\n",
    "        return 'finite'\n",
    "    else:\n",
    "        return '-'\n",
    "    \n",
    "def identify_tense(char):\n",
    "    if char == 'f':\n",
    "        return 'future'\n",
    "    elif char == 'p':\n",
    "        return 'past'\n",
    "    elif char == 'n':\n",
    "        return 'present'\n",
    "    else:\n",
    "        return '-'\n",
    "    \n",
    "def identify_aspect(char):\n",
    "    if char == 'p':\n",
    "        return 'perfect'\n",
    "    elif char == 'o':\n",
    "        return 'progressive'\n",
    "    elif char == 'b':\n",
    "        return 'both perfect and progressive'\n",
    "    else:\n",
    "        return '-'\n",
    "    \n",
    "def identify_person(char):\n",
    "    if char == '3':\n",
    "        return '3rd person'\n",
    "    else:\n",
    "        return '-'\n",
    "    \n",
    "def identify_voice(char):\n",
    "    if char == 'a':\n",
    "        return 'active'\n",
    "    elif char == 'p':\n",
    "        return 'passive'\n",
    "    else:\n",
    "        return '-'\n",
    "\n",
    "def parse_inflection(inflection):\n",
    "    keys = [\"form\",\"tense\",\"aspect\",\"person\",\"voice\"]\n",
    "    inflection = [char for char in inflection]\n",
    "    print(inflection)\n",
    "    inflection_dict = {}\n",
    "    for i in range(len(keys)):\n",
    "        inflection_dict[keys[i]] = globals()[f\"identify_{keys[i]}\"](inflection[i])\n",
    "    return inflection_dict\n",
    "\n",
    "def parse_arguments(arguments):\n",
    "    argument_dict = {}\n",
    "    for arg in arguments:\n",
    "        arg = arg.split(\"-\")\n",
    "        argument_dict['-'.join(arg[1:]).strip()] = arg[0]\n",
    "    return argument_dict\n",
    "\n",
    "def parse_row(row):\n",
    "    row = row.split(' ')\n",
    "    ann_dict ={}\n",
    "    ann_dict[\"wsj_filepath\"] = row[0]\n",
    "    ann_dict[\"sen_id\"] = int(row[1])\n",
    "    ann_dict[\"pred_loc\"] = int(row[2])\n",
    "    ann_dict[\"tagger\"] = row[3] \n",
    "    ann_dict[\"frameset\"] = row[4]\n",
    "    ann_dict[\"inflection\"] = parse_inflection(row[5])\n",
    "    ann_dict[\"arguments\"] = parse_arguments(row[6:])\n",
    "    return ann_dict\n",
    "\n",
    "def gen_ptb_file_path(file_path):\n",
    "    file = re.sub(r'wsj/[0-9]{2}/', '', file_path)\n",
    "    return \"../LDC-Data/NLTK-PTB-Sample/\" + file\n",
    "\n",
    "def type_propbank_ann(ann):\n",
    "    if re.match('^[0-9]+:[0-9]+$', ann) is not None:\n",
    "        return 1\n",
    "    elif re.match('^[0-9]+:[0-9]+\\*[0-9]+:[0-9]+$', ann) is not None:\n",
    "        return 2\n",
    "    elif re.match('^[0-9]+:[0-9]+,[0-9]+:[0-9]+$', ann) is not None:\n",
    "        return 3\n",
    "    else:\n",
    "        return 4\n",
    "\n",
    "def get_tokens_for_type_one_ann(index, leaves):\n",
    "    leaf = int(index.split(\":\")[0])\n",
    "    height = int(index.split(\":\")[1])\n",
    "    terminal = leaves[int(index.split(':')[0])]\n",
    "    height = int(index.split(':')[1])\n",
    "    if height == 0:\n",
    "        tokens = [terminal.token] \n",
    "    else:\n",
    "        parent_node = terminal\n",
    "        for i in range(0, height):\n",
    "            parent_node = parent_node.parent\n",
    "        tokens = apply_f_to_leaves_spanned_by_subtree(parent_node, return_token)\n",
    "    return tokens\n",
    "    \n",
    "def gen_propbank_labels(row):\n",
    "    #Annotation Dictionary\n",
    "    ann_dict = parse_row(row)\n",
    "    #Tree\n",
    "    file_path = gen_ptb_file_path(ann_dict[\"wsj_filepath\"])\n",
    "    tree = generate_ptb_parse_tree(file_path, ann_dict[\"sen_id\"])\n",
    "    #Tokens and Leaves of Tree\n",
    "    tokens = apply_f_to_leaves_spanned_by_subtree(tree, return_token)\n",
    "    leaves = apply_f_to_leaves_spanned_by_subtree(tree, return_node)\n",
    "    #Labels \n",
    "    label_dict = {key:None for key in tokens}\n",
    "    argument_dict = ann_dict[\"arguments\"]\n",
    "    for label, indices in argument_dict.items():\n",
    "        ann_type = type_propbank_ann(indices)\n",
    "        if ann_type == 1:\n",
    "            tokens = get_tokens_for_type_one_ann(indices, leaves)\n",
    "            for tok in tokens:\n",
    "                label_dict[tok] = label\n",
    "    print(label_dict)\n",
    "            \n",
    "            \n",
    "\n",
    "\n",
    "with open(\"../LDC-Data/LDC2004T14/propbank_1/data/prop.txt\") as f:\n",
    "    for line in f:\n",
    "        print(line)\n",
    "        gen_propbank_labels(line)\n",
    "        break\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "59ed1739",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['( (S \\n', '    (NP-SBJ (NNP Mr.) (NNP Vinken) )\\n', '    (VP (VBZ is) \\n', '      (NP-PRD \\n', '        (NP (NN chairman) )\\n', '        (PP (IN of) \\n', '          (NP \\n', '            (NP (NNP Elsevier) (NNP N.V.) )\\n', '            (, ,) \\n', '            (NP (DT the) (NNP Dutch) (VBG publishing) (NN group) )))))\\n', '    (. .) ))\\n']\n",
      "[['(', '(', 'S'], ['(', 'NP-SBJ ', '(', 'NNP Mr.', ')', '(', 'NNP Vinken', ')', ')'], ['(', 'VP ', '(', 'VBZ is', ')'], ['(', 'NP-PRD'], ['(', 'NP ', '(', 'NN chairman', ')', ')'], ['(', 'PP ', '(', 'IN of', ')'], ['(', 'NP'], ['(', 'NP ', '(', 'NNP Elsevier', ')', '(', 'NNP N.V.', ')', ')'], ['(', ', ,', ')'], ['(', 'NP ', '(', 'DT the', ')', '(', 'NNP Dutch', ')', '(', 'VBG publishing', ')', '(', 'NN group', ')', ')', ')', ')', ')', ')'], ['(', '. .', ')', ')', ')']]\n",
      "['(', 'S', '(', 'NP-SBJ ', '(', 'NNP Mr.', ')', '(', 'NNP Vinken', ')', ')', '(', 'VP ', '(', 'VBZ is', ')', '(', 'NP-PRD', '(', 'NP ', '(', 'NN chairman', ')', ')', '(', 'PP ', '(', 'IN of', ')', '(', 'NP', '(', 'NP ', '(', 'NNP Elsevier', ')', '(', 'NNP N.V.', ')', ')', '(', ', ,', ')', '(', 'NP ', '(', 'DT the', ')', '(', 'NNP Dutch', ')', '(', 'VBG publishing', ')', '(', 'NN group', ')', ')', ')', ')', ')', ')', '(', '. .', ')', ')']\n",
      "Mr.\n",
      "Vinken\n",
      "is\n",
      "chairman\n",
      "of\n",
      "Elsevier\n",
      "N.V.\n",
      ",\n",
      "the\n",
      "Dutch\n",
      "publishing\n",
      "group\n",
      ".\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "754ce705",
   "metadata": {},
   "outputs": [],
   "source": [
    "        '''\n",
    "        print(\"Node (by label) Removed\")\n",
    "        print(node.label)\n",
    "        print(\"Nodes Remaining (by label) in Unvisited List\")\n",
    "        for i in unvisited:\n",
    "            print(i.label)\n",
    "        '''\n",
    "            #print(\"My Node is a leaf node\")\n",
    "            '''\n",
    "            print(\"Token associated with leaf node\")\n",
    "            print(node.token)\n",
    "            '''\n",
    "            #print(\"My Node is not a leaf node\")\n",
    "            '''\n",
    "            print(\"Node's children\")\n",
    "            for child in children:\n",
    "                print(child.label)\n",
    "            '''\n",
    "                '''\n",
    "                print(\"Initial Nodes (by label) in Unvisited List\")\n",
    "                for i in unvisited:\n",
    "                    print(i.label)\n",
    "                '''\n",
    "                '''\n",
    "                print(\"Nodes (by label) in Unvisited List after Insert\")\n",
    "                for i in unvisited:\n",
    "                    print(i.label)\n",
    "                '''"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
